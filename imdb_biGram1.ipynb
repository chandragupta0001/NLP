{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPighgAQSvwR+WR9hFZ2SRB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTdyIpMlmedL"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import nltk"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJS4pk4rmtes",
        "outputId": "8b298075-13f2-4a82-fbd5-cd4906d2076a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdvw4YAPmuZx",
        "outputId": "a1cd9618-9cc9-42bb-c271-7ff430fac5f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive',force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhdrts9IuO65"
      },
      "source": [
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "# load doc\n",
        "  doc = load_doc(filename)\n",
        "# clean doc\n",
        "  tokens = clean_doc(doc)\n",
        "# update counts\n",
        "  vocab.update(tokens)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO4Z38HAuEug"
      },
      "source": [
        "def load_doc(filename):\n",
        "# open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "# read all text\n",
        "  text = file.read()\n",
        "# close the file\n",
        "  file.close()\n",
        "  return text"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAM8ynp-m8QQ"
      },
      "source": [
        "def clean_doc(doc):\n",
        "# split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "# filter out stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  # filter out short tokens\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  # bi grams\n",
        "  bitokens=[]\n",
        "  for i in range(len(tokens)-1):\n",
        "    bitokens.append(tokens[i]+\" \"+tokens[i+1])\n",
        "  return bitokens+tokens"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otpU09k3t-m8"
      },
      "source": [
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "# walk through all files in the folder\n",
        "   for filename in listdir(directory):\n",
        "# skip any reviews in the test set\n",
        "     if filename.startswith('cv9'):\n",
        "       continue\n",
        "# cre ate the full path of the file to open\n",
        "     path = directory + '/' + filename\n",
        "# add doc to vocab\n",
        "     add_doc_to_vocab(path, vocab)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuJkTW-4tEVN",
        "outputId": "1b2dfe38-bf99-46c5-b97f-056c6b5d90b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "process_docs('/gdrive/My Drive/datasets/txt_sentoken/pos', vocab)\n",
        "process_docs('/gdrive/My Drive/datasets/txt_sentoken/neg', vocab)\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "526627\n",
            "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('could', 1248), ('bad', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5_zbI-5tHeL",
        "outputId": "a2365f4d-92e2-4617-abf2-52d6965607eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(vocab.most_common(200))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('could', 1248), ('bad', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936), ('seems', 930), ('back', 923), ('made', 912), ('theres', 901), ('work', 901), ('makes', 883), ('however', 874), ('years', 867), ('world', 864), ('every', 858), ('big', 854), ('though', 843), ('better', 818), ('enough', 812), ('take', 802), ('seen', 801), ('around', 801), ('performance', 795), ('real', 789), ('role', 784), ('going', 782), ('audience', 781), ('gets', 780), ('isnt', 773), ('think', 773), ('may', 773), ('things', 763), ('actually', 760), ('look', 744), ('last', 741), ('funny', 738), ('comedy', 738), ('almost', 728), ('fact', 723), ('played', 721), ('thing', 718), ('nothing', 713), ('say', 712), ('although', 711), ('right', 710), ('thats', 709), ('since', 699), ('come', 699), ('find', 692), ('script', 685), ('plays', 682), ('long', 681), ('cast', 674), ('john', 667), ('old', 662), ('ever', 661), ('comes', 654), ('young', 648), ('without', 640), ('show', 639), ('actors', 639), ('part', 635), ('least', 613), ('lot', 612), ('takes', 604), ('acting', 603), ('original', 600), ('point', 591), ('away', 588), ('star', 585), ('goes', 584), ('quite', 582), ('course', 572), ('might', 571), ('cant', 571), ('family', 571), ('minutes', 568), ('three', 567), ('must', 566), ('place', 562), ('rather', 562), ('im', 562), ('interesting', 561), ('anything', 557), ('screen', 555), ('effects', 554), ('guy', 554), ('far', 544), ('day', 535), ('yet', 533), ('watch', 526), ('year', 523), ('didnt', 523), ('seem', 523), ('times', 518), ('instead', 512), ('sense', 510), ('fun', 509), ('always', 509), ('picture', 509), ('special', 507), ('home', 505), ('give', 505), ('trying', 504), ('making', 504), ('bit', 500), ('kind', 498), ('job', 494), ('want', 493), ('wife', 492), ('series', 485), ('american', 481), ('becomes', 480), ('along', 476), ('pretty', 476), ('set', 475), ('men', 474), ('together', 473), ('help', 472), ('probably', 470), ('woman', 467), ('become', 467), ('actor', 460), ('hard', 456), ('everything', 456), ('money', 455), ('hollywood', 455), ('given', 451), ('gives', 450), ('dialogue', 448), ('whole', 447), ('sure', 446), ('high', 445), ('black', 443), ('watching', 437), ('wants', 436), ('got', 426), ('death', 425), ('music', 422), ('feel', 421), ('perhaps', 418), ('play', 411), ('moments', 411), ('next', 411), ('especially', 410), ('less', 405), ('done', 405), ('everyone', 404), ('james', 398), ('different', 397), ('city', 396), ('looks', 395), ('sex', 394), ('simply', 392), ('completely', 389)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIJvErxSxMOH",
        "outputId": "ba45f839-7936-4815-b41a-9e3d3b380b6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(vocab.most_common()[:-50-1:-1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('guncrazy', 1), ('secondclass', 1), ('belch', 1), ('yucked', 1), ('associated see', 1), ('resumes associated', 1), ('blot resumes', 1), ('embarrassment blot', 1), ('life embarrassment', 1), ('otherwise life', 1), ('olds otherwise', 1), ('lawrence year', 1), ('ages murphy', 1), ('flawlessly ages', 1), ('baker flawlessly', 1), ('legend rick', 1), ('makeup legend', 1), ('artistry makeup', 1), ('virtue artistry', 1), ('lifes redeeming', 1), ('protest lifes', 1), ('without protest', 1), ('portrayals without', 1), ('accept portrayals', 1), ('audiences accept', 1), ('projects audiences', 1), ('greenlight projects', 1), ('studios greenlight', 1), ('scripts studios', 1), ('actors eat', 1), ('fault rest', 1), ('demeaning fault', 1), ('caricatures demeaning', 1), ('sets caricatures', 1), ('youths sets', 1), ('sexcrazed youths', 1), ('guncrazy sexcrazed', 1), ('hiphop guncrazy', 1), ('artists hiphop', 1), ('scam artists', 1), ('conniving scam', 1), ('fasttalking conniving', 1), ('presented fasttalking', 1), ('blacks presented', 1), ('tuckers blacks', 1), ('murphy chris', 1), ('actors murphy', 1), ('creeping movies', 1), ('stereotype creeping', 1), ('insidious stereotype', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQfduayUtJMT",
        "outputId": "7f959aeb-6b7f-4361-d6a4-36917ee065e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "min_occurane = 2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
        "print(len(tokens))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "81474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH28R5mGs9dd"
      },
      "source": [
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "# convert lines to a single blob of text\n",
        "  data = '\\n'.join(lines)\n",
        "# open file\n",
        "  file = open(filename, 'w')\n",
        "# write text\n",
        "  file.write(data)\n",
        "# close file\n",
        "  file.close()\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, '/gdrive/My Drive/datasets/txt_sentoken/vocab_bi.txt')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2vuq3n-s4T6"
      },
      "source": [
        "def doc_to_line(filename, vocab):\n",
        "# load the doc\n",
        "   doc = load_doc(filename)\n",
        "# clean doc\n",
        "   tokens = clean_doc(doc)\n",
        "  #  print(tokens) \n",
        "# filter by vocab\n",
        "   tokens = [w for w in tokens if w in vocab]\n",
        "   \n",
        "   return ' '.join(tokens)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnOl1wELs0GY"
      },
      "source": [
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "  lines = list()\n",
        "# walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "# skip any reviews in the test set\n",
        "    if is_train and filename.startswith('cv9'):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith('cv9'):\n",
        "      continue\n",
        "# create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "# load and clean the doc\n",
        "    line = doc_to_line(path, vocab)\n",
        "    # print(line)\n",
        "# add to list\n",
        "    lines.append(line)\n",
        "  return lines\n",
        "\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab,is_train):\n",
        "# load documents\n",
        "  neg = process_docs('/gdrive/My Drive/datasets/txt_sentoken/neg', vocab,is_train)\n",
        "  pos = process_docs('/gdrive/My Drive/datasets/txt_sentoken/pos', vocab,is_train)\n",
        "  docs = neg + pos\n",
        "# prepare labels\n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "  return docs, labels\n",
        "\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "def create_tokenizer(lines):\n",
        "   tokenizer = Tokenizer()\n",
        "   tokenizer.fit_on_texts(lines)\n",
        "   return tokenizer"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8XQ4bCjzhnJ"
      },
      "source": [
        "vocab_filename = '/gdrive/My Drive/datasets/txt_sentoken/vocab_bi.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASdt4oCWzlEx",
        "outputId": "941ea584-7d5e-431b-9fae-59257320ec2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='binary')\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='binary')\n",
        "print(Xtrain.shape, Xtest.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1800, 25768) (200, 25768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toJ36rZLzw2v",
        "outputId": "9eed23ba-d798-454e-f237-686073cc944e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "Xtrain"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 1., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 1., 1., ..., 0., 0., 0.],\n",
              "       [0., 1., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VStvAnJM0Ou8"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiJPfDkj0VFH"
      },
      "source": [
        "n_words = Xtest.shape[1]\n",
        "# define the model\n",
        "def define_model(n_words):\n",
        "# define network\n",
        "  model = Sequential()\n",
        "  model.add(Dense(64, input_shape=(n_words,), activation='relu'))\n",
        "  # model.add(Dense(128,activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "# compile network\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# summarize defined model\n",
        "  # model.summary()\n",
        "  # plot_model(model, to_file='model.png', show_shapes=True)\n",
        "  return model"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeOAs-0p0bu3",
        "outputId": "b49739ee-974f-4314-a3c0-137382e2934e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "model=define_model(n_words)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                1649216   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 1,649,281\n",
            "Trainable params: 1,649,281\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxKODb7m1jMf"
      },
      "source": [
        "import numpy as np\n",
        "Xtrain=np.array(Xtrain)\n",
        "ytrain=np.array(ytrain)\n",
        "ytest=np.array(ytest)\n",
        "Xtest=np.array(Xtest)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQIlkP9o0hZ4",
        "outputId": "963a7206-8b39-4ac1-a450-15c106eb663c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "model.fit(Xtrain,ytrain,epochs=10,validation_split=0.2)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.5145 - accuracy: 0.7410 - val_loss: 0.5628 - val_accuracy: 0.7056\n",
            "Epoch 2/10\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 0.0824 - accuracy: 0.9931 - val_loss: 0.7778 - val_accuracy: 0.6361\n",
            "Epoch 3/10\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 0.5479 - val_accuracy: 0.7472\n",
            "Epoch 4/10\n",
            "45/45 [==============================] - 1s 15ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.6811 - val_accuracy: 0.7111\n",
            "Epoch 5/10\n",
            "45/45 [==============================] - 1s 15ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.7029 - val_accuracy: 0.7111\n",
            "Epoch 6/10\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.7150 - val_accuracy: 0.7139\n",
            "Epoch 7/10\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.7500 - val_accuracy: 0.7111\n",
            "Epoch 8/10\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.7956 - val_accuracy: 0.7000\n",
            "Epoch 9/10\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 7.2294e-04 - accuracy: 1.0000 - val_loss: 0.8244 - val_accuracy: 0.7056\n",
            "Epoch 10/10\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 5.2164e-04 - accuracy: 1.0000 - val_loss: 0.8667 - val_accuracy: 0.6972\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f80d0ad6940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2SmyV6u1V4_",
        "outputId": "353d8d5a-88ae-469d-c2ea-03f8c875a801",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(Xtest,ytest)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3457 - accuracy: 0.8700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.34567201137542725, 0.8700000047683716]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7tafTAP2D7X"
      },
      "source": [
        "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
        "  scores = list()\n",
        "  n_repeats = 10\n",
        "  n_words = Xtest.shape[1]\n",
        "  for i in range(n_repeats):\n",
        "    model=define_model(n_words)\n",
        "    model.fit(Xtrain, ytrain, epochs=10, verbose=0)\n",
        "    loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "    scores.append(acc)\n",
        "    print('%d accuracy: %s' % ((i+1), acc))\n",
        "  return scores"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZQDnlqH8P95",
        "outputId": "db8b18a8-1c30-41ad-c00c-a683b7853a01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "evaluate_mode(Xtrain,ytrain,Xtest,ytest)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 accuracy: 0.9200000166893005\n",
            "2 accuracy: 0.9049999713897705\n",
            "3 accuracy: 0.9350000023841858\n",
            "4 accuracy: 0.9350000023841858\n",
            "5 accuracy: 0.925000011920929\n",
            "6 accuracy: 0.9449999928474426\n",
            "7 accuracy: 0.9300000071525574\n",
            "8 accuracy: 0.925000011920929\n",
            "9 accuracy: 0.9300000071525574\n",
            "10 accuracy: 0.9200000166893005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9200000166893005,\n",
              " 0.9049999713897705,\n",
              " 0.9350000023841858,\n",
              " 0.9350000023841858,\n",
              " 0.925000011920929,\n",
              " 0.9449999928474426,\n",
              " 0.9300000071525574,\n",
              " 0.925000011920929,\n",
              " 0.9300000071525574,\n",
              " 0.9200000166893005]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw2ZVKL-8aW0"
      },
      "source": [
        "def prepare_data(train_docs, test_docs, mode):\n",
        "# create the tokenizer\n",
        "  tokenizer = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "  tokenizer.fit_on_texts(train_docs)\n",
        "# encode training data set\n",
        "  Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
        "# encode training data set\n",
        "  Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
        "  return Xtrain, Xtest"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRXgDBcm9pzu"
      },
      "source": [
        "from pandas import DataFrame\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX-0Ri189sMs",
        "outputId": "a8e329ea-da1d-4e70-98e8-36096b2b5fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "# run experiment\n",
        "modes = ['binary', 'count', 'tfidf', 'freq']\n",
        "results = DataFrame()\n",
        "for mode in modes:\n",
        "  print(mode)\n",
        "# prepare data for mode\n",
        "  Xtrain, Xtest = prepare_data(train_docs, test_docs, mode)\n",
        "# evaluate model on data for mode\n",
        "  results[mode] = evaluate_mode(Xtrain, ytrain, Xtest, ytest)\n",
        "# summarize results\n",
        "print(results.describe())\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "binary\n",
            "1 accuracy: 0.9300000071525574\n",
            "2 accuracy: 0.9200000166893005\n",
            "3 accuracy: 0.9300000071525574\n",
            "4 accuracy: 0.9350000023841858\n",
            "5 accuracy: 0.9350000023841858\n",
            "6 accuracy: 0.925000011920929\n",
            "7 accuracy: 0.9150000214576721\n",
            "8 accuracy: 0.9300000071525574\n",
            "9 accuracy: 0.925000011920929\n",
            "10 accuracy: 0.9300000071525574\n",
            "count\n",
            "1 accuracy: 0.8999999761581421\n",
            "2 accuracy: 0.8949999809265137\n",
            "3 accuracy: 0.8999999761581421\n",
            "4 accuracy: 0.8949999809265137\n",
            "5 accuracy: 0.8999999761581421\n",
            "6 accuracy: 0.9100000262260437\n",
            "7 accuracy: 0.9100000262260437\n",
            "8 accuracy: 0.9049999713897705\n",
            "9 accuracy: 0.8999999761581421\n",
            "10 accuracy: 0.8899999856948853\n",
            "tfidf\n",
            "1 accuracy: 0.8600000143051147\n",
            "2 accuracy: 0.8600000143051147\n",
            "3 accuracy: 0.875\n",
            "4 accuracy: 0.8650000095367432\n",
            "5 accuracy: 0.8650000095367432\n",
            "6 accuracy: 0.8799999952316284\n",
            "7 accuracy: 0.8650000095367432\n",
            "8 accuracy: 0.8949999809265137\n",
            "9 accuracy: 0.8799999952316284\n",
            "10 accuracy: 0.8899999856948853\n",
            "freq\n",
            "1 accuracy: 0.8700000047683716\n",
            "2 accuracy: 0.8799999952316284\n",
            "3 accuracy: 0.8399999737739563\n",
            "4 accuracy: 0.8700000047683716\n",
            "5 accuracy: 0.8799999952316284\n",
            "6 accuracy: 0.8650000095367432\n",
            "7 accuracy: 0.875\n",
            "8 accuracy: 0.8700000047683716\n",
            "9 accuracy: 0.8799999952316284\n",
            "10 accuracy: 0.8700000047683716\n",
            "          binary      count      tfidf       freq\n",
            "count  10.000000  10.000000  10.000000  10.000000\n",
            "mean    0.927500   0.900500   0.873500   0.870000\n",
            "std     0.006346   0.006433   0.012483   0.011785\n",
            "min     0.915000   0.890000   0.860000   0.840000\n",
            "25%     0.925000   0.896250   0.865000   0.870000\n",
            "50%     0.930000   0.900000   0.870000   0.870000\n",
            "75%     0.930000   0.903750   0.880000   0.878750\n",
            "max     0.935000   0.910000   0.895000   0.880000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6dQA7zB9vUI",
        "outputId": "a742c5b5-f58a-44d0-e283-b252dd750a00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "results.boxplot()\n",
        "pyplot.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-ksHYiH_Pxb"
      },
      "source": [
        "def predict_sentiment(review, vocab, tokenizer, model):\n",
        "# clean\n",
        "  tokens = clean_doc(review)\n",
        "# filter by vocab\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "# convert to line\n",
        "  line = ' '.join(tokens)\n",
        "# encode\n",
        "  encoded = tokenizer.texts_to_matrix([line], mode='binary')\n",
        "# predict sentiment\n",
        "  yhat = model.predict(encoded, verbose=0)\n",
        "# retrieve predicted percentage and label\n",
        "  percent_pos = yhat[0,0]\n",
        "  print(percent_pos)\n",
        "  if round(percent_pos) == 0:\n",
        "    return (1-percent_pos), 'NEGATIVE'\n",
        "  return percent_pos, 'POSITIVE'"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfAO8PAS_YME",
        "outputId": "d9967820-8c57-4ec2-cc63-63b2288a290b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "review=\"i had great exceptation, but i found it not upto the mark\"\n",
        "predict_sentiment(review, vocab, tokenizer, model)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.54321784\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.54321784, 'POSITIVE')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90Vkln7P_q3r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}