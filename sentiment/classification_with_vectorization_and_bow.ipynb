{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list=[]\n",
    "for root,dirs,files in os.walk(\"C:\\\\Users\\\\Chandragupta\\\\Desktop\\\\sentiment_classification\\\\aclImdb\"):\n",
    "    for name in files:\n",
    "        path = os.path.join(root,name)\n",
    "        if path.endswith(\"txt\"):\n",
    "            path_list.append(path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Chandragupta\\\\Desktop\\\\sentiment_classification\\\\aclImdb\\\\test\\\\neg\\\\0_2.txt',\n",
       " 'C:\\\\Users\\\\Chandragupta\\\\Desktop\\\\sentiment_classification\\\\aclImdb\\\\test\\\\neg\\\\10000_4.txt',\n",
       " 'C:\\\\Users\\\\Chandragupta\\\\Desktop\\\\sentiment_classification\\\\aclImdb\\\\test\\\\neg\\\\10001_1.txt',\n",
       " 'C:\\\\Users\\\\Chandragupta\\\\Desktop\\\\sentiment_classification\\\\aclImdb\\\\test\\\\neg\\\\10002_3.txt',\n",
       " 'C:\\\\Users\\\\Chandragupta\\\\Desktop\\\\sentiment_classification\\\\aclImdb\\\\test\\\\neg\\\\10003_3.txt',\n",
       " 'C:\\\\Users\\\\Chandragupta\\\\Desktop\\\\sentiment_classification\\\\aclImdb\\\\test\\\\neg\\\\10004_2.txt',\n",
       " 'C:\\\\Users\\\\Chandragupta\\\\Desktop\\\\sentiment_classification\\\\aclImdb\\\\test\\\\neg\\\\10005_2.txt',\n",
       " 'C:\\\\Users\\\\Chandragupta\\\\Desktop\\\\sentiment_classification\\\\aclImdb\\\\test\\\\neg\\\\10006_2.txt',\n",
       " 'C:\\\\Users\\\\Chandragupta\\\\Desktop\\\\sentiment_classification\\\\aclImdb\\\\test\\\\neg\\\\10007_4.txt',\n",
       " 'C:\\\\Users\\\\Chandragupta\\\\Desktop\\\\sentiment_classification\\\\aclImdb\\\\test\\\\neg\\\\10008_4.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "random.shuffle(path_list)\n",
    "random.shuffle(path_list)\n",
    "random.shuffle(path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\Chandragupta\\\\Desktop\\\\sentiment_classification\\\\aclImdb\\\\test\\\\neg\\\\8952_1.txt', 'C:\\\\Users\\\\Chandragupta\\\\Desktop\\\\sentiment_classification\\\\aclImdb\\\\train\\\\pos\\\\2301_10.txt', 'C:\\\\Users\\\\Chandragupta\\\\Desktop\\\\sentiment_classification\\\\aclImdb\\\\train\\\\neg\\\\6040_1.txt', 'C:\\\\Users\\\\Chandragupta\\\\Desktop\\\\sentiment_classification\\\\aclImdb\\\\train\\\\pos\\\\9321_8.txt', 'C:\\\\Users\\\\Chandragupta\\\\Desktop\\\\sentiment_classification\\\\aclImdb\\\\test\\\\pos\\\\5333_10.txt', 'C:\\\\Users\\\\Chandragupta\\\\Desktop\\\\sentiment_classification\\\\aclImdb\\\\train\\\\neg\\\\8999_4.txt', 'C:\\\\Users\\\\Chandragupta\\\\Desktop\\\\sentiment_classification\\\\aclImdb\\\\test\\\\neg\\\\5821_3.txt', 'C:\\\\Users\\\\Chandragupta\\\\Desktop\\\\sentiment_classification\\\\aclImdb\\\\train\\\\neg\\\\637_3.txt', 'C:\\\\Users\\\\Chandragupta\\\\Desktop\\\\sentiment_classification\\\\aclImdb\\\\test\\\\pos\\\\7104_7.txt', 'C:\\\\Users\\\\Chandragupta\\\\Desktop\\\\sentiment_classification\\\\aclImdb\\\\test\\\\neg\\\\10504_1.txt']\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(path_list[:10])\n",
    "print(len(path_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "y=[]\n",
    "\n",
    "for p in path_list:\n",
    "    myfile=open(p, encoding=\"utf8\")\n",
    "    X.append(myfile.read())\n",
    "    cat=p.split('\\\\');\n",
    "    c=(cat[7]==\"pos\")*1\n",
    "    y.append(c)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "669"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(output_sequence_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=tf.data.Dataset.from_tensor_slices(X)\n",
    "vectorize_layer.adapt(d.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182736"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_example = tf.constant([\"It's a great, great movie! I loved it. you watch too\", \"It was terrible, run away!!!\",\"hi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords(keras.layers.Layer):\n",
    "    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):\n",
    "        super().__init__(dtype=tf.int32, **kwargs)\n",
    "        self.n_tokens = n_tokens\n",
    "    def call(self, inputs):\n",
    "        one_hot = tf.one_hot(inputs, self.n_tokens)\n",
    "        return tf.reduce_sum(one_hot, axis=1)[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagofwords=BagOfWords(n_tokens=1002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "model.add(vectorize_layer)\n",
    "model.add(bagofwords)\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 15000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 180s 7ms/sample - loss: 0.3849 - accuracy: 0.8316 - val_loss: 0.3468 - val_accuracy: 0.8481\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 168s 7ms/sample - loss: 0.2873 - accuracy: 0.8764 - val_loss: 0.3580 - val_accuracy: 0.8399\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 162s 6ms/sample - loss: 0.2112 - accuracy: 0.9131 - val_loss: 0.4741 - val_accuracy: 0.8253\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 168s 7ms/sample - loss: 0.1140 - accuracy: 0.9567 - val_loss: 0.4454 - val_accuracy: 0.8460\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 162s 6ms/sample - loss: 0.0572 - accuracy: 0.9796 - val_loss: 0.6386 - val_accuracy: 0.8421\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 164s 7ms/sample - loss: 0.0413 - accuracy: 0.9858 - val_loss: 0.7618 - val_accuracy: 0.8464\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 161s 6ms/sample - loss: 0.0320 - accuracy: 0.9890 - val_loss: 0.9085 - val_accuracy: 0.8341\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 163s 7ms/sample - loss: 0.0312 - accuracy: 0.9894 - val_loss: 0.7901 - val_accuracy: 0.8407\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 163s 7ms/sample - loss: 0.0234 - accuracy: 0.9925 - val_loss: 0.8733 - val_accuracy: 0.8392\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 163s 7ms/sample - loss: 0.0200 - accuracy: 0.9938 - val_loss: 1.0590 - val_accuracy: 0.8431\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 162s 6ms/sample - loss: 0.0232 - accuracy: 0.9924 - val_loss: 0.9168 - val_accuracy: 0.8409\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 165s 7ms/sample - loss: 0.0148 - accuracy: 0.9949 - val_loss: 0.9270 - val_accuracy: 0.8389\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 164s 7ms/sample - loss: 0.0146 - accuracy: 0.9949 - val_loss: 1.1292 - val_accuracy: 0.8421\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 166s 7ms/sample - loss: 0.0151 - accuracy: 0.9948 - val_loss: 1.0953 - val_accuracy: 0.8393\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 164s 7ms/sample - loss: 0.0189 - accuracy: 0.9934 - val_loss: 1.0061 - val_accuracy: 0.8452\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 166s 7ms/sample - loss: 0.0132 - accuracy: 0.9958 - val_loss: 1.1267 - val_accuracy: 0.8401\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 163s 7ms/sample - loss: 0.0133 - accuracy: 0.9956 - val_loss: 1.0291 - val_accuracy: 0.8459\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 162s 6ms/sample - loss: 0.0097 - accuracy: 0.9970 - val_loss: 1.1891 - val_accuracy: 0.8466\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 162s 6ms/sample - loss: 0.0134 - accuracy: 0.9952 - val_loss: 1.1631 - val_accuracy: 0.8443\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 162s 6ms/sample - loss: 0.0152 - accuracy: 0.9955 - val_loss: 1.0306 - val_accuracy: 0.8458\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X[:25000],y[:25000],epochs=20,validation_data=(X[25000:40000],y[25000:40000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 39s 4ms/sample - loss: 1.0078 - accuracy: 0.8491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0077831466689706, 0.8491]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X[40000:],y[40000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_example=[\"good\",\"bad\",\"not bad, good\",\"not boring\",\"boring\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "p=model.predict(X_example)\n",
    "for i in p:\n",
    "    if i>.5:\n",
    "        print(\"positive\")\n",
    "    else:\n",
    "        print(\"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When I first saw a glimpse of this movie, I quickly noticed the actress who was playing the role of Lucille Ball. Rachel York\\'s portrayal of Lucy is absolutely awful. Lucille Ball was an astounding comedian with incredible talent. To think about a legend like Lucille Ball being portrayed the way she was in the movie is horrendous. I cannot believe out of all the actresses in the world who could play a much better Lucy, the producers decided to get Rachel York. She might be a good actress in other roles but to play the role of Lucille Ball is tough. It is pretty hard to find someone who could resemble Lucille Ball, but they could at least find someone a bit similar in looks and talent. If you noticed York\\'s portrayal of Lucy in episodes of I Love Lucy like the chocolate factory or vitavetavegamin, nothing is similar in any way-her expression, voice, or movement.<br /><br />To top it all off, Danny Pino playing Desi Arnaz is horrible. Pino does not qualify to play as Ricky. He\\'s small and skinny, his accent is unreal, and once again, his acting is unbelievable. Although Fred and Ethel were not similar either, they were not as bad as the characters of Lucy and Ricky.<br /><br />Overall, extremely horrible casting and the story is badly told. If people want to understand the real life situation of Lucille Ball, I suggest watching A&E Biography of Lucy and Desi, read the book from Lucille Ball herself, or PBS\\' American Masters: Finding Lucy. If you want to see a docudrama, \"Before the Laughter\" would be a better choice. The casting of Lucille Ball and Desi Arnaz in \"Before the Laughter\" is much better compared to this. At least, a similar aspect is shown rather than nothing.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Chandragupta\\\\Desktop\\\\sentiment_classification\\\\aclImdb\\\\train\\\\neg\\\\120_1.txt'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radar",
   "language": "python",
   "name": "radar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
